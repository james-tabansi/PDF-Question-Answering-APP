## load the module used to access the key stored in the .env
from dotenv import load_dotenv
import os
#to host and build the api
import streamlit as st

#import module to work with the pdf
from PyPDF2 import PdfReader #to read the text inside the pdf

#use the langchain framework to build apps powered by LLM
#to split the texts inside the pdf into smaller chunks
from langchain.text_splitter import CharacterTextSplitter

#to get the word-embeddings for the tokens using openai embeddings
from langchain.embeddings.openai import OpenAIEmbeddings

#to create vectors from the embeddings
from langchain.vectorstores import FAISS

#load the model
from langchain.chains.question_answering import load_qa_chain
from langchain.llms import OpenAI

#load the api Key and keep it private from users
load_dotenv(dotenv_path = '.env')


##Build the app interface
#set the title
st.title("PDF Document Understanding Application")
st.header("Ask Your PDF")

pdf = st.file_uploader("Upload your pdf", type='pdf')

#check if uploaded file is not None
if pdf is not None:
    #read the pdf
    pdf_reader = PdfReader(pdf)

    #extract the text from the pdf reader
    #step1: create an empty string
    text = ''
    #step2: go through each pages of the pdf, extract the text
    #and append to the string
    for page in pdf_reader.pages:
        text += page.extract_text()

    #split the text into tokens so it is not too large for the model to process
    #'\n' uses the new line character as the separator
    text_splitter = CharacterTextSplitter(
        separator="\n",
        chunk_size=1000, 
        chunk_overlap=200,
        length_function=len
        )

    #pass the text to the splitter
    tok = text_splitter.split_text(text)

    #create embeddings
    embeddings = OpenAIEmbeddings()

    #create a knowledge base. this knowledge is like a vocabulary
    #it
    knowledge_base = FAISS.from_texts(tok, embeddings)

    #get the user to give prompts or questions on the pdf
    query = st.text_input("Ask questions about your PDF")

    if query is not None:
        #if user inputs a query
        #take the query and search the knowledge base for content related to user quuery
        information = knowledge_base.similarity_search(query)

        #pass the found information to the openai model
        #initialize the model
        model = OpenAI() #here you can specify the type of model

        #load the question answering chain using the openai model
        chain = load_qa_chain(llm=model, chain_type="stuff")

        #run the chain with the information and the user query to generate a response
        response = chain.run(input_documents=information, question=query)

        #display the response generated by the openai model as a success
        st.success(response)




    
    



